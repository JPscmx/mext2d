{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#some deprecation notices\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#set a seed so results are replicable \n",
    "import random\n",
    "random.seed(121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_from_df_cv(data, class_name):\n",
    "\n",
    "    \"\"\"\n",
    "    For classifying with cross-validation\n",
    "    data(df): Pandas dataframe with all data, with last column containing the classification labels and all \n",
    "                others as features. Row names are sampleIDs.    \n",
    "    class_name(str): name of the classification column\n",
    "\n",
    "    Returns: \n",
    "        X (df): the 16S data\n",
    "        y (Series): the factored classes\n",
    "        factor_index: the factorized index for each class\n",
    "        features (list): list of 16S feature names\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    features = df.columns[:-1]\n",
    "    df[\"factored\"], factor_index = pd.factorize(df[class_name])\n",
    "    X, y = df[features], df[\"factored\"]\n",
    "    \n",
    "    return X, y, factor_index, features\n",
    "\n",
    "def prepare_data_from_df(data, class_name, train_split=0.75):\n",
    "    \"\"\"\n",
    "    For classifying without cross-validation\n",
    "    data(df): Pandas dataframe with all data, with last column containing the classification labels and all \n",
    "                others as features. Row names are sampleIDs.\n",
    "    train_split(float): percent of data to use as training set (default 0.75)\n",
    "    \n",
    "    class_name(str): name of the classification column\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    features = df.columns[:-1]\n",
    "    df[\"factored\"], factor_index = pd.factorize(df[class_name])\n",
    "    \n",
    "    df['is_train'] = np.random.uniform(0, 1, data.shape[0]) <= train_split\n",
    "    train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "    Xtrain, Xtest, ytrain, ytest = train[features], test[features], train[\"factored\"], test[\"factored\"]\n",
    "    \n",
    "    return Xtrain, Xtest, ytrain, ytest, factor_index, features\n",
    "    \n",
    "def train_and_test_classifier(Xtrain, Xtest, ytrain, ytest, features, max_features, n_estimators=len(features)):\n",
    "    \"\"\"\n",
    "    Xtrain(df): training data - row names are sampleIDs\n",
    "    Xtest(df): test data - row names are sampleIDs\n",
    "    ytrain(df): correct classification of training samples (factorized) - generally last column of a dataset\n",
    "    ytest(df): correct classification of test samples (factorized) - generally last column of a dataset\n",
    "    \n",
    "    Returns:\n",
    "        clf(RandomForestClassifier): the fitted classifier\n",
    "        acc: the accuracy of the classifier\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, max_features=max_features)\n",
    "    clf.fit(Xtrain[features], ytrain)\n",
    "    acc = clf.score(Xtest[features], ytest) \n",
    "    \n",
    "    return clf, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_analysis(data, metadata, runs=100, cv=10, feature_runs=100, num_features=5, class_name=\"diabetes\"):\n",
    "    \"\"\"\n",
    "    Produces in the current directory 1) a text file of average AUC for each class comparison \n",
    "    (T2D vs NGT, IGT vs NGT, T2D vs IGT), 2) a table of the most important num_features features in each of feature_runs runs,\n",
    "    for each class comparison, with the importance of each feature for that run.\n",
    "    \n",
    "    data (df): otu table (samples, features), with a descriptive data.index.name\n",
    "    metadata (df): clinical data (samples, features)\n",
    "    \n",
    "    Returns: data.index.name\n",
    "    \"\"\"\n",
    "    \n",
    "    #prepare the data, making sure that there are the same samples in the 16S data and the metadata\n",
    "    data.index = data.index.map(str)\n",
    "    metadata = metadata[metadata.index.isin(data.index)]\n",
    "    data = data[data.index.isin(metadata.index)]\n",
    "    data_classifier = data.copy()\n",
    "    #get the diabetes classification for each sample in the 16S data\n",
    "    data_classifier[\"diabetes\"] = metadata[\"diabetes\"]\n",
    "    \n",
    "    #separate the different classes of samples\n",
    "    NGT = data_classifier[data_classifier[\"diabetes\"]==\"NGT\"]\n",
    "    T2D = data_classifier[data_classifier[\"diabetes\"]==\"T2D\"]\n",
    "    IGT = data_classifier[data_classifier[\"diabetes\"]==\"IGT\"]\n",
    "    T2D.index.name = \"T2D\"\n",
    "    NGT.index.name = \"NGT\"\n",
    "    IGT.index.name = \"IGT\"    \n",
    "    \n",
    "    with open(\"{}_mean_auc_cv10.txt\".format(data.index.name), \"w\") as auc_txt:\n",
    "        for class_pair in [[T2D, NGT_T2D_sample], [T2D, IGT_T2D_sample], [IGT, NGT_IGT_sample]]:\n",
    "            with open(\"{}_{}_{}_important_features_cv10.csv\".format(data.index.name, class_pair[0].index.name, class_pair[1].index.name), \"w\") as features_txt:\n",
    "                cv_auc = []\n",
    "                data_sample = pd.concat(class_pair)\n",
    "                X,y, factor_index, features = prepare_data_from_df_cv(data_sample, class_name)                \n",
    "                #take the average AUC across the runs (with cross-validation)\n",
    "                for i in range(runs):\n",
    "                    clf = RandomForestClassifier(n_jobs=-1, n_estimators=len(features), max_features=len(features))                    \n",
    "                    cv_auc.append(cross_val_score(clf, X, y, cv=cv,n_jobs=-1, scoring=make_scorer(roc_auc_score)))\n",
    "                auc_txt.write(\"{} vs {}: {}\".format(class_pair[0].index.name, class_pair[1].index.name, np.mean(cv_auc)))\n",
    "                auc_txt.write(\"\\n\")\n",
    "                important_features = pd.DataFrame()\n",
    "                #getting the most important features for each run (no cross-validation), with a different split each time\n",
    "                for i in range(feature_runs):\n",
    "                    Xtrain, Xtest, ytrain, ytest, factor_index, features = prepare_data_from_df(data_sample, class_name)\n",
    "                    clf, acc = train_and_test_classifier(Xtrain, Xtest, ytrain, ytest, features, len(features))\n",
    "                    important_features = important_features.append(pd.DataFrame(index=features, columns=[\"importance\"], data=clf.feature_importances_).sort_values(by=\"importance\", ascending=False).head())\n",
    "                important_features.to_csv(\"{}_{}_{}_important_features.csv\".format(data.index.name, class_pair[0].index.name, class_pair[1].index.name))   \n",
    "    return data.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variants = pd.read_csv(\"../christien_tables/DESeq_normalized_counts/counts_norm_variant.csv\", index_col=0)\n",
    "variants.index.name = \"variants\"\n",
    "genera = pd.read_csv(\"../christien_tables/DESeq_normalized_counts/counts_norm_genera.csv\", index_col=0)\n",
    "genera.index.name = \"genera\"\n",
    "family = pd.read_csv(\"../abundances_by_tax_level/Family_otu_table.csv\", index_col=0).T\n",
    "family.index.name = \"family\"\n",
    "sparcc = pd.read_csv(\"../sparcc/sparcc_clustered_otu.csv\", index_col=0).T \n",
    "sparcc.index.name = \"sparcc\"\n",
    "sparcc = sparcc/sparcc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [family, sparcc, genera, variants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#classifying diabetic status based on WHO standards\n",
    "metadata = pd.read_csv(\"../metadata_variable_filtered/metadata_for_pcoa_num_only_nosum.csv\", index_col=0) #read metadata file\n",
    "metadata.index = metadata.index.map(str)\n",
    "metadata[\"diabetes\"] = np.zeros\n",
    "for index in metadata.index:\n",
    "    fasting = metadata.loc[index, \"glucose_0\"]\n",
    "    twohr = metadata.loc[index, \"glucose_120\"]\n",
    "    if fasting >= 126 or twohr >= 200:\n",
    "        metadata[\"diabetes\"][index] = \"T2D\"\n",
    "    else:\n",
    "        if twohr >= 140 and twohr < 200:\n",
    "            metadata[\"diabetes\"][index] = \"IGT\"\n",
    "        else:\n",
    "            metadata[\"diabetes\"][index] = \"NGT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family\n",
      "sparcc\n",
      "genera\n"
     ]
    }
   ],
   "source": [
    "for df in data:\n",
    "    print(classifier_analysis(df, metadata, runs=100, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
